#!/usr/bin/env python3
"""
YOLOv5 + DeepSORT ì„±ëŠ¥ ë¹„êµ (Before/After CLIP)
- Before: YOLOv5 + DeepSORT (ê¸°ë³¸)
- After: YOLOv5 + DeepSORT + CLIP (ê°œì„ )
"""

import cv2
import numpy as np
import torch
import clip
from PIL import Image
import sys
import argparse
from pathlib import Path
import time
import json
from datetime import datetime

# YOLOv5 ê²½ë¡œ ì¶”ê°€
YOLO_PATH = Path(__file__).parent / "yolov5"
if str(YOLO_PATH) not in sys.path:
    sys.path.insert(0, str(YOLO_PATH))

# DeepSORT ê²½ë¡œ ì¶”ê°€
DEEPSORT_PATH = Path(__file__).parent / "deep_sort"
if str(DEEPSORT_PATH) not in sys.path:
    sys.path.insert(0, str(DEEPSORT_PATH))

# YOLOv5 imports
sys.path.insert(0, str(Path(__file__).parent / "yolov5"))
from models.common import DetectMultiBackend
from utils.torch_utils import select_device
from utils.general import non_max_suppression, scale_boxes
from utils.dataloaders import LoadImages

# DeepSORT imports
from deep_sort import tracker
from deep_sort import nn_matching
from deep_sort.detection import Detection

PERSON_CLASS = 0


class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ ì¸¡ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.frame_times = []
        self.detection_counts = []
        self.track_counts = []
        self.track_ids = set()
        self.total_tracks = 0
        self.id_switches = 0
        self.fragments = 0
        self.motp = 0.0  # Multiple Object Tracking Precision
        self.mota = 0.0  # Multiple Object Tracking Accuracy
        
    def add_frame(self, frame_time, detection_count, track_count, current_track_ids):
        """í”„ë ˆì„ë³„ ì§€í‘œ ì¶”ê°€"""
        self.frame_times.append(frame_time)
        self.detection_counts.append(detection_count)
        self.track_counts.append(track_count)
        
        # ID ìŠ¤ìœ„ì¹˜ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
        if hasattr(self, 'prev_track_ids'):
            new_tracks = current_track_ids - self.prev_track_ids
            lost_tracks = self.prev_track_ids - current_track_ids
            self.id_switches += len(new_tracks) + len(lost_tracks)
        
        self.prev_track_ids = current_track_ids.copy()
        self.track_ids.update(current_track_ids)
        self.total_tracks = len(self.track_ids)
    
    def calculate_metrics(self):
        """ìµœì¢… ì§€í‘œ ê³„ì‚°"""
        if not self.frame_times:
            return {}
        
        return {
            'avg_fps': 1.0 / np.mean(self.frame_times),
            'avg_frame_time_ms': np.mean(self.frame_times) * 1000,
            'avg_detections': np.mean(self.detection_counts),
            'avg_tracks': np.mean(self.track_counts),
            'total_tracks': self.total_tracks,
            'id_switches': self.id_switches,
            'fragments': self.fragments,
            'total_frames': len(self.frame_times)
        }


def extract_clip_embedding(image, bbox, clip_model, clip_preprocess, device):
    """CLIP ì„ë² ë”© ì¶”ì¶œ"""
    try:
        x1, y1, x2, y2 = map(int, bbox)
        h, w = image.shape[:2]
        x1 = max(0, min(x1, w-1))
        y1 = max(0, min(y1, h-1))
        x2 = max(x1+1, min(x2, w))
        y2 = max(y1+1, min(y2, h))
        
        cropped_image = image[y1:y2, x1:x2]
        pil_image = Image.fromarray(cropped_image)
        image_tensor = clip_preprocess(pil_image).unsqueeze(0).to(device)
        
        with torch.no_grad():
            embedding = clip_model.encode_image(image_tensor)
            embedding = embedding / embedding.norm(dim=-1, keepdim=True)
        
        return embedding.cpu().numpy().flatten().astype(np.float32)
    except Exception as e:
        print(f"CLIP ì„ë² ë”© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None


def run_baseline_tracking(source, weights='yolov5s.pt', conf_thres=0.4, iou_thres=0.4):
    """ê¸°ë³¸ YOLOv5 + DeepSORT (CLIP ì—†ìŒ)"""
    print("ğŸ”µ ê¸°ë³¸ YOLOv5 + DeepSORT ì‹¤í–‰ ì¤‘...")
    
    device = select_device('')
    model = DetectMultiBackend(weights, device=device, dnn=False, data=None, fp16=False)
    
    # DeepSORT íŠ¸ë˜ì»¤ (CLIP ì—†ìŒ)
    metric = nn_matching.NearestNeighborDistanceMetric("cosine", 0.2)
    tracker_obj = tracker.Tracker(
        metric, 
        max_iou_distance=0.7, 
        max_age=30, 
        n_init=3,
        max_clip_distance=1.0  # CLIP ë§¤ì¹­ ë¹„í™œì„±í™”
    )
    
    dataset = LoadImages(source, img_size=640, stride=32, auto=True)
    metrics = PerformanceMetrics()
    
    for path, im, im0s, vid_cap, s in dataset:
        start_time = time.time()
        
        # YOLO ì¶”ë¡ 
        im = torch.from_numpy(im).to(device)
        im = im.half() if model.fp16 else im.float()
        im /= 255
        if len(im.shape) == 3:
            im = im[None]
        
        pred = model(im, augment=False, visualize=False)
        pred = non_max_suppression(
            pred, conf_thres, iou_thres, 
            classes=[PERSON_CLASS],
            max_det=1000
        )
        
        # Detection ê°ì²´ ìƒì„± (ê¸°ë³¸ feature ì‚¬ìš©)
        detections = []
        im0 = im0s.copy()
        
        if len(pred[0]):
            det = pred[0]
            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()
            
            # ì‘ì€ ê°ì²´ í•„í„°ë§
            img_area = im0.shape[0] * im0.shape[1]
            bbox_area = (det[:, 2] - det[:, 0]) * (det[:, 3] - det[:, 1])
            area_ratio = bbox_area / img_area
            min_area_ratio = 0.005
            valid_objects = area_ratio >= min_area_ratio
            det = det[valid_objects]
            
            for *xyxy, conf, cls in det:
                x1, y1, x2, y2 = xyxy
                w = x2 - x1
                h = y2 - y1
                # CUDA í…ì„œë¥¼ CPUë¡œ ì´ë™ í›„ NumPy ë³€í™˜
                tlwh = np.array([x1.cpu(), y1.cpu(), w.cpu(), h.cpu()], dtype=np.float64)
                
                # ê¸°ë³¸ feature (ëœë¤ ë²¡í„°)
                feature = np.random.randn(128).astype(np.float32)
                
                detection = Detection(tlwh, float(conf.cpu()), feature)
                detections.append(detection)
        
        # DeepSORT ì—…ë°ì´íŠ¸
        tracker_obj.predict()
        tracker_obj.update(detections)
        
        # ì§€í‘œ ìˆ˜ì§‘
        frame_time = time.time() - start_time
        current_track_ids = {t.track_id for t in tracker_obj.tracks if t.is_confirmed()}
        metrics.add_frame(frame_time, len(detections), len(current_track_ids), current_track_ids)
    
    return metrics.calculate_metrics()


def run_clip_tracking(source, weights='yolov5s.pt', conf_thres=0.4, iou_thres=0.4):
    """ê°œì„ ëœ YOLOv5 + DeepSORT + CLIP"""
    print("ğŸŸ¢ YOLOv5 + DeepSORT + CLIP ì‹¤í–‰ ì¤‘...")
    
    device = select_device('')
    model = DetectMultiBackend(weights, device=device, dnn=False, data=None, fp16=False)
    
    # CLIP ëª¨ë¸ ë¡œë“œ
    clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
    clip_model.eval()
    
    # DeepSORT íŠ¸ë˜ì»¤ (CLIP í¬í•¨)
    metric = nn_matching.NearestNeighborDistanceMetric("cosine", 0.2)
    tracker_obj = tracker.Tracker(
        metric, 
        max_iou_distance=0.7, 
        max_age=30, 
        n_init=3,
        max_clip_distance=0.5  # CLIP ë§¤ì¹­ í™œì„±í™”
    )
    
    dataset = LoadImages(source, img_size=640, stride=32, auto=True)
    metrics = PerformanceMetrics()
    
    for path, im, im0s, vid_cap, s in dataset:
        start_time = time.time()
        
        # YOLO ì¶”ë¡ 
        im = torch.from_numpy(im).to(device)
        im = im.half() if model.fp16 else im.float()
        im /= 255
        if len(im.shape) == 3:
            im = im[None]
        
        pred = model(im, augment=False, visualize=False)
        pred = non_max_suppression(
            pred, conf_thres, iou_thres, 
            classes=[PERSON_CLASS],
            max_det=1000
        )
        
        # Detection ê°ì²´ ìƒì„± (CLIP ì„ë² ë”© ì‚¬ìš©)
        detections = []
        im0 = im0s.copy()
        
        if len(pred[0]):
            det = pred[0]
            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()
            
            # ì‘ì€ ê°ì²´ í•„í„°ë§
            img_area = im0.shape[0] * im0.shape[1]
            bbox_area = (det[:, 2] - det[:, 0]) * (det[:, 3] - det[:, 1])
            area_ratio = bbox_area / img_area
            min_area_ratio = 0.005
            valid_objects = area_ratio >= min_area_ratio
            det = det[valid_objects]
            
            for *xyxy, conf, cls in det:
                bbox = [int(x) for x in xyxy]
                
                # CLIP ì„ë² ë”© ì¶”ì¶œ
                clip_embedding = extract_clip_embedding(
                    im0, bbox, clip_model, clip_preprocess, device
                )
                
                if clip_embedding is not None:
                    x1, y1, x2, y2 = bbox
                    w = x2 - x1
                    h = y2 - y1
                    # ì´ë¯¸ intë¡œ ë³€í™˜ëœ bboxì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    tlwh = np.array([x1, y1, w, h], dtype=np.float64)
                    
                    # CLIP ì„ë² ë”©ì„ featureë¡œ ì‚¬ìš©
                    feature = clip_embedding
                    
                    detection = Detection(tlwh, float(conf.cpu()), feature, clip_embedding)
                    detections.append(detection)
        
        # DeepSORT ì—…ë°ì´íŠ¸
        tracker_obj.predict()
        tracker_obj.update(detections)
        
        # ì§€í‘œ ìˆ˜ì§‘
        frame_time = time.time() - start_time
        current_track_ids = {t.track_id for t in tracker_obj.tracks if t.is_confirmed()}
        metrics.add_frame(frame_time, len(detections), len(current_track_ids), current_track_ids)
    
    return metrics.calculate_metrics()


def print_comparison_table(baseline_metrics, clip_metrics):
    """ë¹„êµ í…Œì´ë¸” ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ (Before vs After)")
    print("="*80)
    
    # ë©”íŠ¸ë¦­ ì´ë¦„ê³¼ ë‹¨ìœ„
    metrics_info = [
        ("í‰ê·  FPS", "fps", "avg_fps"),
        ("í‰ê·  í”„ë ˆì„ ì‹œê°„", "ms", "avg_frame_time_ms"),
        ("í‰ê·  íƒì§€ ìˆ˜", "ê°œ", "avg_detections"),
        ("í‰ê·  íŠ¸ë™ ìˆ˜", "ê°œ", "avg_tracks"),
        ("ì´ íŠ¸ë™ ìˆ˜", "ê°œ", "total_tracks"),
        ("ID ìŠ¤ìœ„ì¹˜", "íšŒ", "id_switches"),
        ("ì´ í”„ë ˆì„", "ê°œ", "total_frames")
    ]
    
    print(f"{'ì§€í‘œ':<20} {'Before (ê¸°ë³¸)':<15} {'After (CLIP)':<15} {'ê°œì„ ìœ¨':<15}")
    print("-"*80)
    
    for name, unit, key in metrics_info:
        before_val = baseline_metrics.get(key, 0)
        after_val = clip_metrics.get(key, 0)
        
        if before_val > 0:
            if key in ['avg_frame_time_ms', 'id_switches']:
                # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ
                improvement = ((before_val - after_val) / before_val) * 100
                symbol = "â†“" if improvement > 0 else "â†‘"
            else:
                # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ
                improvement = ((after_val - before_val) / before_val) * 100
                symbol = "â†‘" if improvement > 0 else "â†“"
        else:
            improvement = 0
            symbol = "="
        
        print(f"{name:<20} {before_val:<15.2f} {after_val:<15.2f} {symbol}{abs(improvement):.1f}%")
    
    print("="*80)


def save_comparison_results(baseline_metrics, clip_metrics, save_path):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    results = {
        'timestamp': datetime.now().isoformat(),
        'baseline_metrics': baseline_metrics,
        'clip_metrics': clip_metrics,
        'comparison': {}
    }
    
    # ê°œì„ ìœ¨ ê³„ì‚°
    for key in baseline_metrics:
        if key in clip_metrics and baseline_metrics[key] > 0:
            if key in ['avg_frame_time_ms', 'id_switches']:
                improvement = ((baseline_metrics[key] - clip_metrics[key]) / baseline_metrics[key]) * 100
            else:
                improvement = ((clip_metrics[key] - baseline_metrics[key]) / baseline_metrics[key]) * 100
            results['comparison'][key] = {
                'improvement_percent': improvement,
                'baseline': baseline_metrics[key],
                'clip': clip_metrics[key]
            }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {save_path}")


def main():
    parser = argparse.ArgumentParser(description='YOLOv5 + DeepSORT ì„±ëŠ¥ ë¹„êµ')
    parser.add_argument('--source', type=str, required=True, help='ì…ë ¥ ì˜ìƒ ê²½ë¡œ')
    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='YOLOv5 ëª¨ë¸ ê°€ì¤‘ì¹˜')
    parser.add_argument('--conf-thres', type=float, default=0.4, help='ì‹ ë¢°ë„ ì„ê³„ê°’')
    parser.add_argument('--iou-thres', type=float, default=0.4, help='NMS IOU ì„ê³„ê°’')
    parser.add_argument('--save-results', type=str, default='results/performance_comparison.json', help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    print("ğŸš€ YOLOv5 + DeepSORT ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    print(f"ì…ë ¥ ì˜ìƒ: {args.source}")
    print(f"ëª¨ë¸: {args.weights}")
    print(f"ì‹ ë¢°ë„ ì„ê³„ê°’: {args.conf_thres}")
    print(f"IOU ì„ê³„ê°’: {args.iou_thres}")
    print("-"*50)
    
    # Before: ê¸°ë³¸ YOLOv5 + DeepSORT
    baseline_metrics = run_baseline_tracking(
        args.source, args.weights, args.conf_thres, args.iou_thres
    )
    
    print("\n" + "-"*50)
    
    # After: YOLOv5 + DeepSORT + CLIP
    clip_metrics = run_clip_tracking(
        args.source, args.weights, args.conf_thres, args.iou_thres
    )
    
    # ê²°ê³¼ ë¹„êµ ì¶œë ¥
    print_comparison_table(baseline_metrics, clip_metrics)
    
    # ê²°ê³¼ ì €ì¥
    save_path = Path(args.save_results)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    save_comparison_results(baseline_metrics, clip_metrics, save_path)
    
    print("\nâœ… ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ!")


if __name__ == '__main__':
    main()
